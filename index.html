<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Boyuan Jiang(姜博源)</title>
    <meta name="author" content="Boyuan  Jiang" />
    <meta name="description" content="Boyuan Jiang(姜博源)'s personal website. Based on [*folio](https://github.com/bogoli/-folio) design.
" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/icon.ico"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://byjiang.com/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           Boyuan Jiang(姜博源)
          </h1>
          <p class="desc">Senior Researcher at Tencent Youtu Lab.</p>
        </header>

        <article>
          <div class="profile float-right">
<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid z-dept-1 rounded" src="/assets/img/prof_pic.jpg" alt="prof_pic.jpg">

  </picture>

</figure>

          </div>

          <div class="clearfix">
            <p>I am a senior researcher at Tencent Youtu Lab, where I work on computer vision and machine learning. Recently, I am working for developing high-fidelity virtual try-on model for Tencent Cloud.</p>

<p>I got my B.A. from the Harbin Institute of Technology(HIT) in 2017 and got my M.A. degree from the Zhejiang University(ZJU) in 2020. I have ever worked at NetEase, SenseTime and Hikvision as research intern and joined Tencent Youtu Lab in 2020.</p>

          </div>

          <!-- News -->          
          <div class="news">
            <h2>news</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless"> 
                <tr>
                  <th scope="row">11/2024</th>
                  <td>
                    We released <a href="https://byjiang.com/FitDiT/">FitDiT</a>, a high-fidelity virtual try-on work based on SD3.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">10/2024</th>
                  <td>
                    We released <a href="https://github.com/BoyuanJiang/FluxFit" target="_blank" rel="noopener noreferrer">FluxFit</a>, a virtual try-on work based on FLUX.1-dev.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">02/2024</th>
                  <td>
                    One paper about fast identity-preserved personalization accepted by CVPR’24.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">12/2023</th>
                  <td>
                    One paper about video action recognition accepted by AAAI’24.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">09/2023</th>
                  <td>
                    One paper about video frame interpolation accepted by IEEE Transactions on Image Processing.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">07/2022</th>
                  <td>
                    One paper about image colorization accepted by ECCV’22.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">03/2022</th>
                  <td>
                    One paper about video frame interpolation accepted by CVPR’22.

 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">03/2021</th>
                  <td>
                    Our Team <b>Imagination</b> is the winner of CVPR NTIRE 2021 Challenge on Video Spatial-Temporal Super-Resolution.

 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">12/2020</th>
                  <td>
                    One paper about action recognition accepted by AAAI’21.

 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">04/2020</th>
                  <td>
                    I joined Tencent Youtu Lab.

 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">03/2020</th>
                  <td>
                    I graduated from Zhejiang University.

 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">02/2020</th>
                  <td>
                    One paper about domain adaption accepted by CVPR’20.

 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">07/2019</th>
                  <td>
                    One paper about action recognition accepted by ICCV’19.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">11/2018</th>
                  <td>
                    One paper about unsupervised domain adaption accepted by AAAI’19.

 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

          <!-- Selected papers -->
          <div class="publications">
            <h2>selected publications</h2>
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3"><img class="teaser" src="/assets/img/paper_teasers/colorformer.jpg"></div>

        <!-- Entry bib key -->
        <div id="ji2022ECCV" class="col-sm-7">
        
          <!-- Title -->
          <div class="title">ColorFormer: Image Colorization via Color Memory assisted Hybrid-attention Transformer</div>
          <!-- Author -->
          <div class="author">Ji, Xiaozhong*, <b>Jiang, Boyuan*</b>, Luo, Donghao, Tao, Guangpin, Chu, Wenqing, Xie, Zhifeng, Wang, Chengjie, and Tai, Ying
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>European Conference on Computer Vision <b>(ECCV)</b></em> <em>2022</em>
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/ColorFormer.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Automatic image colorization is a challenging task that attracts a lot of research interest. Previous methods employing deep neural networks have produced impressive results. However, these colorization images are still unsatisfactory and far from practical applications. The reason is that semantic consistency and color richness are two key elements ignored by existing methods. In this work, we propose an automatic image colorization method via color memory assisted hybrid-attention transformer, namely ColorFormer. Our network consists of a transformer-based encoder and a color memory decoder. The core module of the encoder is our proposed global-local hybrid attention operation, which improves the ability to capture global receptive field dependencies. With the strong power to model contextual semantic information of grayscale image in different scenes, our network can produce semantic-consistent colorization results. In decoder part, we design a color memory module which stores various semantic-color mapping for image-adaptive queries. The queried color priors are used as reference to help the decoder produce more vivid and diverse results. Experimental results show that our method can generate more realistic and semantically matched color images compared with state-of-the-art methods. Moreover, owing to the proposed end-to-end architecture, the inference speed reaches 40 FPS on a V100 GPU, which meets the real-time requirement.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3"><img class="teaser" src="/assets/img/paper_teasers/IFRNet.jpg"></div>

        <!-- Entry bib key -->
        <div id="kong2022IFRNet" class="col-sm-7">
        
          <!-- Title -->
          <div class="title">IFRNet: Intermediate Feature Refine Network for Efficient Frame Interpolation</div>
          <!-- Author -->
          <div class="author">Kong, Lingtong*, <b>Jiang, Boyuan*</b>, Luo, Donghao, Chu, Wenqing, Huang, Feiyue, Tai, Ying, Wang, Chengjie, and Yang, Jie
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Computer Vision and Pattern Recognition <b>(CVPR)</b></em> <em>2022</em>
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/IFRNet.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://github.com/ltkong218/IFRNet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/ltkong218/IFRNet.svg" alt="GitHub stars" title="">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Prevailing video frame interpolation algorithms, that generate the intermediate frames from consecutive inputs, typically rely on complex model architectures with heavy parameters or large delay, hindering them from diverse real-time applications. In this work, we devise an efficient encoder-decoder based network, termed IFRNet, for fast intermediate frame synthesizing. It first extracts pyramid features from given inputs, and then refines the bilateral intermediate flow fields together with a powerful intermediate feature until generating the desired output. The gradually refined intermediate feature can not only facilitate intermediate flow estimation, but also compensate for contextual details, making IFRNet do not need additional synthesis or refinement module. To fully release its potential, we further propose a novel task-oriented optical flow distillation loss to focus on learning the useful teacher knowledge towards frame synthesizing. Meanwhile, a new geometry consistency regularization term is imposed on the gradually refined intermediate features to keep better structure layout. Experiments on various benchmarks demonstrate the excellent performance and fast inference speed of proposed approaches.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3"><img class="teaser" src="/assets/img/paper_teasers/CMR.jpg"></div>

        <!-- Entry bib key -->
        <div id="wu2021learning" class="col-sm-7">
        
          <!-- Title -->
          <div class="title">Learning Comprehensive Motion Representation for Action Recognition</div>
          <!-- Author -->
          <div class="author">Wu, Mingyu*, <b>Jiang, Boyuan*</b>, Luo, Donghao, Yan, Junchi, Wang, Yabiao, Tai, Ying, Wang, Chengjie, Li, Jilin, Huang, Feiyue, and Yang, Xiaokang
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>AAAI Conference on Artificial Intellige <b>(AAAI)</b></em> <em>2021</em>
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://ojs.aaai.org/index.php/AAAI/article/view/16400" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://ojs.aaai.org/index.php/AAAI/article/view/16400/16207" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/TencentYoutuResearch/ActionRecognition-CMR" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/ActionRecognition-CMR.svg" alt="GitHub stars" title="">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>For action recognition learning, 2D CNN-based methods are efficient but may yield redundant features due to applying the same 2D convolution kernel to each frame. Recent efforts attempt to capture motion information by establishing inter-frame connections while still suffering the limited temporal receptive field or high latency. Moreover, the feature enhancement is often only performed by channel or space dimension in action recognition. To address these issues, we first devise a Channel-wise Motion Enhancement (CME) module to adaptively emphasize the channels related to dynamic information with a channel-wise gate vector. The channel gates generated by CME incorporate the information from all the other frames in the video. We further propose a Spatial-wise Motion Enhancement (SME) module to focus on the regions with the critical target in motion, according to the point-to-point similarity between adjacent feature maps. The intuition is that the change of background is typically slower than the motion area. Both CME and SME have clear physical meaning in capturing action clues. By integrating the two modules into the off-the-shelf 2D network, we finally obtain a Comprehensive Motion Representation (CMR) learning method for action recognition, which achieves competitive performance on Something-Something V1 &amp; V2 and Kinetics-400. On the temporal reasoning datasets Something-Something V1 and V2, our method outperforms the current state-of-the-art by 2.3% and 1.9% when using 16 frames as input, respectively.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3"><img class="teaser" src="/assets/img/paper_teasers/STM.jpg"></div>

        <!-- Entry bib key -->
        <div id="jiang2019stm" class="col-sm-7">
        
          <!-- Title -->
          <div class="title">Stm: Spatiotemporal and motion encoding for action recognition</div>
          <!-- Author -->
          <div class="author">
<b>Jiang, Boyuan</b>, Wang, MengMeng, Gan, Weihao, Wu, Wei, and Yan, Junjie
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>International Conference on Computer Vision <b>(ICCV)</b></em> <em>2019</em>
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_STM_SpatioTemporal_and_Motion_Encoding_for_Action_Recognition_ICCV_2019_paper.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_STM_SpatioTemporal_and_Motion_Encoding_for_Action_Recognition_ICCV_2019_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Spatiotemporal and motion features are two complementary and crucial information for video action recognition. Recent state-of-the-art methods adopt a 3D CNN stream to learn spatiotemporal features and another flow stream to learn motion features. In this work, we aim to efficiently encode these two features in a unified 2D framework. To this end, we first propose a STM block, which contains a Channel-wise SpatioTemporal Module (CSTM) to present the spatiotemporal features and a Channel-wise Motion Module (CMM) to efficiently encode motion features. We then replace original residual blocks in the ResNet architecture with STM blcoks to form a simple yet effective STM network by introducing very limited extra computation cost. Extensive experiments demonstrate that the proposed STM network outperforms the state-of-the-art methods on both temporal-related datasets (i.e., Something-Something v1 &amp; v2 and Jester) and scene-related datasets (i.e., Kinetics-400, UCF-101, and HMDB-51) with the help of encoding spatiotemporal and motion features together.</p>
          </div>
        </div>
      </div>
</li>
</ol>
          </div>

          <!-- Social -->
          <div class="social">
            <div class="contact-icons">
            <a href="mailto:%67%69%6E%67%65%72%31%38%38@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=u62rcKoAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/BoyuanJiang" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
            <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/boyuanjiang.svg" alt="GitHub stars" title="">
            
            </div>

            <div class="contact-note">
              
            </div>
            
          </div>
        </article>
        <div class="clustrmaps">
          <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=080808&amp;w=300&amp;t=tt&amp;d=EjRGPh2Dr65dqTqraYvkUlCl05O1zHSXvOMxpN_2rkQ&amp;co=ffffff&amp;ct=808080&amp;cmo=3acc3a&amp;cmn=ff5353"></script>
        </div>
</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Boyuan  Jiang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.
Last updated: July 13, 2025.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

